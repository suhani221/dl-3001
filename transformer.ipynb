{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6effba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2024-05-10 21:32:29.721907: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-10 21:32:29.721966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-10 21:32:29.723908: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-10 21:32:29.733336: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-10 21:32:30.499366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab93dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(Module):\n",
    "    def __init__(self, alpha=0.5, gamma=2.0, logits=True, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        else:\n",
    "            BCE_loss = torch.nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Preprocessing Data\n",
    "def preprocess_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    text_data = []\n",
    "    for file_path in data['filename']:\n",
    "        file_path = file_path.replace('\\\\', '/')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                text_data.append(text)\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                text = file.read()\n",
    "                text_data.append(text)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    data['text'] = text_data\n",
    "    seizure_types = ['bckg', 'cpsz', 'gnsz', 'fnsz', 'absz', 'tnsz', 'tcsz', 'spsz', 'mysz']\n",
    "    data = data.dropna(subset=['text'])\n",
    "    data[seizure_types] = data[seizure_types].applymap(lambda x: 1 if x > 0 else 0)\n",
    "    return data, seizure_types\n",
    "\n",
    "def split_data(data):\n",
    "    return train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute Metrics for Evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = (logits >= 0).astype(int)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='micro')\n",
    "    recall = recall_score(labels, predictions, average='micro')\n",
    "    f1 = f1_score(labels, predictions, average='micro')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Training the Model with Custom Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = FocalLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.float().view(-1, self.model.config.num_labels))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def fine_tune_model(train_data, val_data, seizure_types):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    train_encodings = tokenizer(list(train_data['text']), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    val_encodings = tokenizer(list(val_data['text']), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    \n",
    "    train_dataset = CustomDataset(train_encodings, train_data[seizure_types].values)\n",
    "    val_dataset = CustomDataset(val_encodings, val_data[seizure_types].values)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",  # Evaluate each epoch\n",
    "        save_strategy=\"epoch\"  # Save model each epoch\n",
    "    )\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(seizure_types))\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    data, seizure_types = preprocess_data(\"seizure_counts.csv\")\n",
    "    train_data, val_data = split_data(data)\n",
    "    fine_tune_model(train_data, val_data, seizure_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18cdea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_364611/2153976013.py:67: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data[seizure_types] = data[seizure_types].applymap(lambda x: 1 if x > 0 else 0)\n",
      "/home/geovisionaries/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CustomDataset object at 0x75ae44d49a80>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geovisionaries/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprachi-parakh\u001b[0m (\u001b[33mperks\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/geovisionaries/prachi/wandb/run-20240510_213245-bl2u1lnt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/perks/huggingface/runs/bl2u1lnt' target=\"_blank\">balmy-bush-11</a></strong> to <a href='https://wandb.ai/perks/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/perks/huggingface' target=\"_blank\">https://wandb.ai/perks/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/perks/huggingface/runs/bl2u1lnt' target=\"_blank\">https://wandb.ai/perks/huggingface/runs/bl2u1lnt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_364611/2153976013.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='286' max='286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [286/286 01:23, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.021862</td>\n",
       "      <td>0.642105</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.019109</td>\n",
       "      <td>0.642105</td>\n",
       "      <td>0.996516</td>\n",
       "      <td>0.706173</td>\n",
       "      <td>0.826590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_364611/2153976013.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/tmp/ipykernel_364611/2153976013.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: [0.86922014 0.13121882 0.20300347 0.29096717 0.14290977 0.1340278\n",
      " 0.15388453 0.1441897  0.12161492]\n",
      "Actual Label: [1 0 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class FocalLoss(Module):\n",
    "    def __init__(self, alpha=0.5, gamma=2.0, logits=True, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        else:\n",
    "            BCE_loss = torch.nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Preprocessing Data\n",
    "def preprocess_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    text_data = []\n",
    "    for file_path in data['filename']:\n",
    "        file_path = file_path.replace('\\\\', '/')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                text_data.append(text)\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                text = file.read()\n",
    "                text_data.append(text)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    data['text'] = text_data\n",
    "    seizure_types = ['bckg', 'cpsz', 'gnsz', 'fnsz', 'absz', 'tnsz', 'tcsz', 'spsz', 'mysz']\n",
    "    data = data.dropna(subset=['text'])\n",
    "    data[seizure_types] = data[seizure_types].applymap(lambda x: 1 if x > 0 else 0)\n",
    "    return data, seizure_types\n",
    "\n",
    "def split_data(data):\n",
    "    return train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute Metrics for Evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute Metrics for Evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = (logits >= 0).astype(int)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='micro')\n",
    "    recall = recall_score(labels, predictions, average='micro')\n",
    "    f1 = f1_score(labels, predictions, average='micro')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Training the Model with Custom Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = FocalLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.float().view(-1, self.model.config.num_labels))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def fine_tune_model(train_data, val_data, seizure_types):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    train_encodings = tokenizer(list(train_data['text']), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    val_encodings = tokenizer(list(val_data['text']), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    \n",
    "    train_dataset = CustomDataset(train_encodings, train_data[seizure_types].values)\n",
    "    print(train_dataset)\n",
    "    val_dataset = CustomDataset(val_encodings, val_data[seizure_types].values)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",  # Evaluate each epoch\n",
    "        save_strategy=\"epoch\"  # Save model each epoch\n",
    "    )\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(seizure_types))\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    predictions = trainer.predict(val_dataset)[0]  # Get only the predictions\n",
    "\n",
    "#     # Print one example from the validation dataset along with its predicted label\n",
    "#     idx = 3  # Change this index to print a different example\n",
    "#     y_pred = torch.sigmoid(torch.tensor(predictions[idx])).detach().cpu().numpy()\n",
    "#     y_actual = val_data[seizure_types].values[idx]\n",
    "#     print(\"Predicted Label:\", y_pred)\n",
    "#     print(\"Actual Label:\", y_actual)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Transformer Encoder model\n",
    "# class Embeddings(nn.Module):\n",
    "#     def __init__(self, d_model, vocab_size):\n",
    "#         super(Embeddings, self).__init__()\n",
    "#         self.emb = nn.Embedding(vocab_size, d_model)\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.emb(x) * math.sqrt(self.d_model)\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "#         pe = torch.zeros(vocab_size, d_model)\n",
    "#         position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(\n",
    "#             torch.arange(0, d_model, 2).float()\n",
    "#             * (-math.log(10000.0) / d_model)\n",
    "#         )\n",
    "\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer(\"pe\", pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.pe[:, : x.size(1), :]\n",
    "#         return self.dropout(x)\n",
    "\n",
    "# class SingleHeadAttention(nn.Module):\n",
    "#     def __init__(self, d_model, d_head_size):\n",
    "#         super().__init__()\n",
    "#         self.lin_key = nn.Linear(d_model, d_head_size, bias=False)\n",
    "#         self.lin_query = nn.Linear(d_model, d_head_size, bias=False)\n",
    "#         self.lin_value = nn.Linear(d_model, d_head_size, bias=False)\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         query = self.lin_query(x)\n",
    "#         key = self.lin_key(x)\n",
    "#         value = self.lin_value(x)\n",
    "\n",
    "#         scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "#         p_attn = scores.softmax(dim=-1)\n",
    "#         x = torch.matmul(p_attn, value)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, h, d_model, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         assert d_model % h == 0\n",
    "#         d_k = d_model // h\n",
    "#         self.multi_head = nn.ModuleList([SingleHeadAttention(d_model, d_k) for _ in range(h)])\n",
    "#         self.lin_agg = nn.Linear(d_model, d_model)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.cat([head(x) for head in self.multi_head], dim=-1)\n",
    "#         return self.lin_agg(x)\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, d_model, eps=1e-6):\n",
    "#         super(LayerNorm, self).__init__()\n",
    "#         self.a_2 = nn.Parameter(torch.ones(d_model))\n",
    "#         self.b_2 = nn.Parameter(torch.zeros(d_model))\n",
    "#         self.eps = eps\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean = x.mean(-1, keepdim=True)\n",
    "#         std = x.std(-1, keepdim=True)\n",
    "#         return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "# class ResidualConnection(nn.Module):\n",
    "#     def __init__(self, d_model, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.norm = LayerNorm(d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         return self.dropout(self.norm(x1 + x2))\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.w_1 = nn.Linear(d_model, d_ff)\n",
    "#         self.w_2 = nn.Linear(d_ff, d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "\n",
    "# class SingleEncoder(nn.Module):\n",
    "#     def __init__(self, d_model, self_attn, feed_forward, dropout):\n",
    "#         super().__init__()\n",
    "#         self.self_attn = self_attn\n",
    "#         self.feed_forward = feed_forward\n",
    "#         self.res_1 = ResidualConnection(d_model, dropout)\n",
    "#         self.res_2 = ResidualConnection(d_model, dropout)\n",
    "\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x_attn = self.self_attn(x)\n",
    "#         x_res_1 = self.res_1(x, x_attn)\n",
    "#         x_ff = self.feed_forward(x_res_1)\n",
    "#         x_res_2 = self.res_2(x_res_1, x_ff)\n",
    "\n",
    "#         return x_res_2\n",
    "\n",
    "# class EncoderBlocks(nn.Module):\n",
    "#     def __init__(self, layer, N):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([layer for _ in range(N)])\n",
    "#         self.norm = LayerNorm(layer.d_model)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "#         return self.norm(x)\n",
    "\n",
    "# class TransformerEncoderModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, nhead, d_ff, N,\n",
    "#                 dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
    "\n",
    "#         self.emb = Embeddings(d_model, vocab_size)\n",
    "#         self.pos_encoder = PositionalEncoding(d_model=d_model, vocab_size=vocab_size)\n",
    "\n",
    "#         attn = MultiHeadAttention(nhead, d_model)\n",
    "#         ff = FeedForward(d_model, d_ff, dropout)\n",
    "#         self.transformer_encoder = EncoderBlocks(SingleEncoder(d_model, attn, ff, dropout), N)\n",
    "#         self.classifier = nn.Linear(d_model, 2)\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.emb(x) * math.sqrt(self.d_model)\n",
    "#         x = self.pos_encoder(x)\n",
    "#         x = self.transformer_encoder(x)\n",
    "#         x = x.mean(dim=1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    data, seizure_types = preprocess_data(\"seizure_counts.csv\")\n",
    "    train_data, val_data = split_data(data)\n",
    "    fine_tune_model(train_data, val_data, seizure_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    data, seizure_types = preprocess_data(\"seizure_counts.csv\")\n",
    "    train_data, val_data = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4030013",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[seizure_types].values[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def remove_seizure_words_in_directory(input_directory, output_directory, keyword=\"seizure\"):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # List all files in the input directory\n",
    "    files = os.listdir(input_directory)\n",
    "    \n",
    "    # Iterate over each file\n",
    "    for file_name in files:\n",
    "        input_file_path = os.path.join(input_directory, file_name)\n",
    "        output_file_path = os.path.join(output_directory, file_name)\n",
    "        \n",
    "        if os.path.isfile(input_file_path) and file_name.endswith('.txt'):\n",
    "            # Read the content of the input file\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "                try:\n",
    "                    content = input_file.read()\n",
    "                except UnicodeDecodeError:\n",
    "                    # If utf-8 decoding fails, try decoding with ISO-8859-1\n",
    "                    with open(input_file_path, 'r', encoding='ISO-8859-1') as alt_input_file:\n",
    "                        content = alt_input_file.read()\n",
    "            \n",
    "            # Remove occurrences of the keyword\n",
    "            cleaned_content = content.replace(keyword, \"\")\n",
    "            \n",
    "            # Write the cleaned content to the output file in the new directory\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(cleaned_content)\n",
    "\n",
    "# Example usage\n",
    "input_directory = \"brain_old\"\n",
    "output_directory = \"brain\"\n",
    "\n",
    "remove_seizure_words_in_directory(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Preprocessing Data\n",
    "def preprocess_data_with_augmentation(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    text_data = []\n",
    "    for file_path in data['filename']:\n",
    "        file_path = file_path.replace('\\\\', '/')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                # Augment text\n",
    "                augmented_text = augment_text(text)\n",
    "                text_data.append(augmented_text)\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                text = file.read()\n",
    "                # Augment text\n",
    "                augmented_text = augment_text(text)\n",
    "                text_data.append(augmented_text)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    data['text'] = text_data\n",
    "    seizure_types = ['bckg', 'cpsz', 'gnsz', 'fnsz', 'absz', 'tnsz', 'tcsz', 'spsz', 'mysz']\n",
    "    data = data.dropna(subset=['text'])\n",
    "#     data[seizure_types] = data[seizure_types].applymap(lambda x: 1 if x > 0 else 0)\n",
    "    data[seizure_types] = data[seizure_types]\n",
    "    return data, seizure_types\n",
    "\n",
    "def split_data(data):\n",
    "    return train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute Metrics for Evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = (logits >= 0).astype(int)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='micro')\n",
    "    recall = recall_score(labels, predictions, average='micro')\n",
    "    f1 = f1_score(labels, predictions, average='micro')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Training the Model with Custom Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.functional.binary_cross_entropy_with_logits\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.float().view(-1, self.model.config.num_labels))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def fine_tune_model(train_data, val_data, seizure_types):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    train_encodings = tokenizer(list(train_data['text']), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    val_encodings = tokenizer(list(val_data['text']), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    \n",
    "    train_dataset = CustomDataset(train_encodings, train_data[seizure_types].values)\n",
    "    val_dataset = CustomDataset(val_encodings, val_data[seizure_types].values)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",  # Evaluate each epoch\n",
    "        save_strategy=\"epoch\"  # Save model each epoch\n",
    "    )\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(seizure_types))\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    data, seizure_types = preprocess_data(\"seizure_counts.csv\")\n",
    "    train_data, val_data = split_data(data)\n",
    "    fine_tune_model(train_data, val_data, seizure_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dcefcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch torchvision\n",
    "!pip install accelerate>=0.21.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f4209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('seizure_counts.csv')\n",
    "# count_columns = data.columns[1:]\n",
    "# data[count_columns] = np.log1p(data[count_columns])\n",
    "# scaler = StandardScaler()\n",
    "# data[count_columns] = scaler.fit_transform(data[count_columns])\n",
    "\n",
    "# def load_text(filename):\n",
    "#     corrected_filename = filename.replace('\\\\', os.sep)\n",
    "#     file_path = os.path.join(os.getcwd(), corrected_filename)  # Replace backslash with forward slash for file path\n",
    "#     try:\n",
    "#         # Try reading with UTF-8 encoding\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     except UnicodeDecodeError:\n",
    "#         # If UTF-8 fails, try a different encoding such as ISO-8859-1\n",
    "#         with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "#             return file.read()\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path}\")\n",
    "#         return \"\"\n",
    "    \n",
    "# data['text'] = data['filename'].apply(load_text)\n",
    "# train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78df97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def preprocess_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    text_data = []\n",
    "    for file_path in data['filename']:\n",
    "        file_path = file_path.replace('\\\\', '/')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                text_data.append(text)\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                text = file.read()\n",
    "                text_data.append(text)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    data['text'] = text_data\n",
    "    seizure_types = ['bckg', 'cpsz', 'gnsz', 'fnsz', 'absz', 'tnsz', 'tcsz', 'spsz', 'mysz']\n",
    "    for seizure_type in seizure_types:\n",
    "        data[seizure_type] = data[seizure_type].apply(lambda x: 1 if x > 0 else 0)\n",
    "    return data, seizure_types\n",
    "\n",
    "def split_data(data):\n",
    "    return train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = (logits >= 0).astype(int)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='micro')\n",
    "    recall = recall_score(labels, predictions, average='micro')\n",
    "    f1 = f1_score(labels, predictions, average='micro')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def fine_tune_model(train_data, val_data, seizure_types):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    train_encodings = tokenizer(list(train_data['text']), truncation=True, padding=True, return_tensors='pt')\n",
    "    val_encodings = tokenizer(list(val_data['text']), truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "    train_dataset = CustomDataset(train_encodings, train_data[seizure_types].values)\n",
    "    val_dataset = CustomDataset(val_encodings, val_data[seizure_types].values)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",  # Match eval and save strategies\n",
    "        save_strategy=\"epoch\",        # Match eval and save strategies\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\"\n",
    "    )\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(seizure_types))\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model\n",
    "\n",
    "# Load CSV and text files\n",
    "csv_file = 'seizure_counts.csv'\n",
    "\n",
    "# Preprocess data\n",
    "data, seizure_types = preprocess_data(csv_file)\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = split_data(data)\n",
    "\n",
    "# Fine-tune model\n",
    "model = fine_tune_model(train_data, val_data, seizure_types)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b393abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def preprocess_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    text_data = []\n",
    "    for file_path in data['filename']:\n",
    "        file_path = file_path.replace('\\\\', '/')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                text_data.append(text)\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                text = file.read()\n",
    "                text_data.append(text)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")  # Append empty string for missing or unreadable files\n",
    "    data['text'] = text_data\n",
    "    \n",
    "    seizure_types = ['bckg', 'cpsz', 'gnsz', 'fnsz', 'absz', 'tnsz', 'tcsz', 'spsz', 'mysz']\n",
    "    for seizure_type in seizure_types:\n",
    "        data[seizure_type] = data[seizure_type].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    return data, seizure_types\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_fn=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def train_model(train_data, val_data, seizure_types):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    train_encodings = tokenizer(list(train_data['text']), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    val_encodings = tokenizer(list(val_data['text']), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "    train_dataset = CustomDataset(train_encodings, train_data[seizure_types].values)\n",
    "    val_dataset = CustomDataset(val_encodings, val_data[seizure_types].values)\n",
    "\n",
    "    pos_weights = torch.tensor([0.1] + [1.0] * (len(seizure_types) - 1))\n",
    "    loss_fn = BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\"\n",
    "    )\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(seizure_types))\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        loss_fn=loss_fn\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model\n",
    "\n",
    "# Load CSV and text files\n",
    "csv_file = 'seizure_counts.csv'\n",
    "\n",
    "# Preprocess data\n",
    "data, seizure_types = preprocess_data(csv_file)\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = train_model(train_data, val_data, seizure_types)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed37419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeizureDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360dcf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_hidden_layers=12, num_attention_heads=12, num_labels=len(count_columns))\n",
    "model = BertForSequenceClassification(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf18414",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SeizureDataset(train_data['text'].tolist(), train_data[count_columns].values)\n",
    "val_dataset = SeizureDataset(val_data['text'].tolist(), val_data[count_columns].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ccff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSLELoss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted, actual):\n",
    "        predicted = torch.relu(predicted)  # Ensuring predictions are non-negative\n",
    "        return torch.mean((torch.log1p(predicted) - torch.log1p(actual)) ** 2)\n",
    "\n",
    "# Initialize MSLE Loss\n",
    "msle_loss = MSLELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "model.train()\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        loss = msle_loss(outputs.logits, labels)\n",
    "        if torch.isnan(loss):\n",
    "            continue  # Skip the batch if loss is nan\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation Phase with corrected data flattening\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        all_preds.extend(outputs.logits.view(-1).cpu().numpy())\n",
    "        all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "# Calculate and print MSE\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "mse = mean_squared_error(all_labels, all_preds)\n",
    "print(f\"Validation Mean Squared Error: {mse}\")\n",
    "\n",
    "# Save the trained model if needed\n",
    "model_path = \"seizure_model\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcce212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `data` is your original DataFrame\n",
    "numeric_data = data.iloc[:, 1:].select_dtypes(include=[np.number])\n",
    "seizure_counts = numeric_data.sum(axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(seizure_counts, bins=30, alpha=0.7, color='blue')\n",
    "plt.title('Distribution of Seizure Counts')\n",
    "plt.xlabel('Seizure Counts')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Descriptive Statistics of Seizure Counts:\")\n",
    "print(seizure_counts.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f1d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `all_preds` and `all_labels` from the model's validation step are available\n",
    "residuals = all_labels - all_preds\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(all_labels, residuals, alpha=0.5)\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Actual Seizure Counts')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert residuals to absolute errors for easier interpretation\n",
    "absolute_errors = np.abs(residuals)\n",
    "sorted_indices = np.argsort(absolute_errors)[::-1]  # Indices of the errors sorted from largest to smallest\n",
    "\n",
    "# Print the texts with the largest errors\n",
    "print(\"Texts with Largest Errors:\")\n",
    "for i in sorted_indices[:5]:  # Change 5 to the number of examples you want to review\n",
    "    print(f\"\\nText:\\n{data.iloc[i]['text']}\")\n",
    "    print(f\"Actual Count: {all_labels[i]}, Predicted Count: {all_preds[i]}, Error: {absolute_errors[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf49ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
